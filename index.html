<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page of the paper 'Towards Practical Capture of High-Fidelity Relightable Avatars'">
  <meta property="og:title" content="TRAvatar"/>
  <meta property="og:description" content="Project page of the paper 'Towards Practical Capture of High-Fidelity Relightable Avatars'"/>
  <meta property="og:url" content="travatar-paper.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/fig_representative.jpg"/>
  <meta property="og:image:width" content="1767"/>
  <meta property="og:image:height" content="1181"/>


  <meta name="twitter:title" content="TRAvatar">
  <meta name="twitter:description" content="Project page of the paper 'Towards Practical Capture of High-Fidelity Relightable Avatars'">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fig_representative.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Towards Practical Capture of High-Fidelity Relightable Avatars</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<style>
  .method-img {
    width: 80%;
    margin-left: auto;
    margin-right:auto;
    display:block;
  }
  .text-center {
    text-align: center;
  }
</style>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Practical Capture of High-Fidelity Relightable Avatars</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Haotian Yang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=MdizB60AAAAJ&hl=en" target="_blank">Mingwu Zheng</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=na2akZYAAAAJ&hl=zh-CN" target="_blank">Wanquan Feng</a><sup>1</sup>,</span>
                    <span class="author-block">
                        <a href="https://brotherhuang.github.io/" target="_blank">Haibin Huang</a><sup>1</sup>,</span>
                        <span class="author-block">
                            <a href="https://users.cs.cf.ac.uk/Yukun.Lai/" target="_blank">Yu-Kun Lai</a><sup>2</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en" target="_blank">Pengfei Wan</a><sup>1</sup>,</span>
                                <span class="author-block">
                                    <a href="https://www.wangzhongyuan.com/" target="_blank">Zhongyuan Wang</a><sup>1</sup>,</span>
                                    <span class="author-block">
                                        <a href="http://www.chongyangma.com/" target="_blank">Chongyang Ma</a><sup>1</sup>,</span>
                                        </span>
                                        </div>

                                        <div class="is-size-5 publication-authors">
                                            <span class="author-block"><sup>1</sup>Kuaishou Technology  <sup>2</sup>Cardiff University<br>SIGGRAPH Asia 2023</span>
                                        </div>

                                        <div class="column has-text-centered">
                                            <div class="publication-links">
                                                <!-- Arxiv PDF link -->
                                            <!-- <span class="link-block">
                                                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                                class="external-link button is-normal is-rounded is-dark">
                                                <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                                </span>
                                                <span>Paper</span>
                                            </a>
                                            </span> -->

                                            <!-- Supplementary PDF link -->
                                            <!-- <span class="link-block">
                                            <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Supplementary</span>
                                            </a> -->
                                        </span>

                                        <!-- Github link -->
                                        <span class="link-block">
                                            <a href="https://github.com/YOUR REPO HERE" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                            <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code(comimg soon!)</span>
                                        </a>
                                        </span>

                                        <!-- ArXiv abstract Link -->
                                        <span class="link-block">
                                            <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <!-- class="carousel results-carousel" -->
        <div id="results-carousel">
         <div class="item">
          <!-- Your image here -->
          <img src="static/images/fig_teaser.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            We present TRAvatar, a novel framework to capture and reconstruct high-fidelity volumetric avatars. Trained efficiently end-to-end on multi-view image sequences under varying illuminations, our virtual avatars can be relighted and animated in real-time of high fidelity.
          </h2>
        </h2>
      </div>
    </div>
  </div>
  </div>
  </section>
  <!-- End image carousel -->
  
  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In this paper, we propose a novel framework, Tracking-free Relightable Avatar (TRAvatar), for capturing and reconstructing high-fidelity 3D avatars. Compared to previous methods, TRAvatar works in a more practical and efficient setting. Specifically, TRAvatar is trained with dynamic image sequences captured in a Light Stage under varying lighting conditions, enabling realistic relighting and real-time animation for avatars in diverse scenes. Additionally, TRAvatar allows for tracking-free avatar capture and obviates the need for accurate surface tracking under varying illumination conditions. Our contributions are two-fold: First, we propose a novel network architecture that explicitly builds on and ensures the satisfaction of the linear nature of lighting. Trained on simple group light captures, TRAvatar can predict the appearance in real-time with a single forward pass, achieving high-quality relighting effects under illuminations of arbitrary environment maps. Second, we jointly optimize the facial geometry and relightable appearance from scratch based on image sequences, where the tracking is implicitly learned. This tracking-free approach brings robustness for establishing temporal correspondences between frames under different lighting conditions. Extensive qualitative and quantitative experiments demonstrate that our framework achieves superior performance for photorealistic avatar animation and relighting.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  
  
  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel">
         <div class="item">
          <!-- Your image here -->
          <img class="method-img" src="static/images/fig_method.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            The pipeline of our framework. TRAvatar is a relightable volumetric avatar representation learned from multiview image sequences, including dynamic expressions and varying illuminations. For each frame, a motion encoder forecasts the disentangled global rigid transformation and expression code. With the given expression code, lighting condition, and view direction, a series of decoders subsequently predict the base mesh and the volumetric primitives mounted on it. Notably, a physically-inspired appearance decoder detailed in is proposed to facilitate network training. Ultimately, the final avatar representation is computed and then rendered, adaptable to any viewpoint and any lighting condition.
          </h2>
        </h2>
      </div>
    </div>
  </div>
  </div>
  </section>
  <!-- End image carousel -->
  
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3 text-center">Video</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            
            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/zAefqIcmIaA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{yang2023towards,
            title={Towards Practical Capture of High-Fidelity Relightable Avatars},
            author={Haotian, Yang and Mingwu, Zheng and Wanquan, Feng and Haibin, Huang and Yu-Kun, Lai and Pengfei, Wan and Zhongyuan, Wang and ChongYang, Ma},
            booktitle={SIGGRAPH Asia 2023 Conference Proceedings},
            year={2023}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
  
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
              <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
  
          </div>
        </div>
      </div>
    </div>
  </footer>
  
  <!-- Statcounter tracking code -->
    
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  
      <!-- End of Statcounter Code -->
  
    </body>
    </html>
  
